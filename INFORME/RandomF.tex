\chapter{Random Forests.}
El algoritmo de Random Forests consiste en un ensamble de árboles de decisión en el que cada uno contiene un bootstrap del set de datos de entrenamiento formado por atributos elegidos de forma aleatoria. El hecho de usar un subset en lugar del set de entrenamiento completo permite evitar overfitting, y el usar subconjuntos de atributos tomados al azar ayuda a determinar cuáles son los atributos que realmente resultan más importantes para la clasificación.

Random forests fue elegido ya que ofrece una gran adaptación a casi cualquier set de datos, brindando una muy buena precisión procesando grandes cantidades de estos. Además posee un muy buen tiempo de ejecución, especialmente teniendo en cuenta su efectividad. 

Este algoritmo tiene dos hiper-parámetros: 
Por un lado la cantidad de árboles: Es fundamental para encontrar un buen funcionamiento hacer una buena elección de los mismos, ya que ante mayor cantidad de árboles, mejor clasificará el algoritmo, pero conllevará una pérdida de performance mayor. 
Por otra parte la cantidad de atributos de cada uno de los árboles:
Se eligieron en la construcción de cada árbol un conjunto al azar de atributos de nuestro set de datos, siendo dicho conjunto de una cantidad de raíz cuadrada de la cantidad total de atributos.

Se realizaron diferentes pruebas en Kaggle aplicando Random Forests al set completo de datos modificando la cantidad de árboles predictores para ver cual es la mas adecuada al problema.
Las pruebas realizadas fueron las siguientes:
\begin{enumerate}
  \item Se probó con una cantidad de 100 árboles que arrojó una efectividad de 96.514%.
  \item Se probó con una cantidad de 1000 árboles que arrojó una efectividad de 96.657%.
  \item Se probó con una cantidad de 1500 árboles que arrojó una efectividad de 96.714%.
  \item Se probó con una cantidad de 2300 árboles que arrojó una efectividad de 96.743%.
  \item Se probó con una cantidad de 3000 árboles que arrojó una efectividad de 96.771%.
\end{enumerate}
(Como antes se dijo siempre se mantuvo el segundo hiperparámetro con el valor de raiz cuadrada de la cantidad total de atributos, que es un valor recomendado para la aplicacion del algoritmo a problemas de clasificación segun la documentacion de la libreria scikit-learn de python)

Se intento correr el algoritmo con una cantidad de 10000 árboles, y luego con una de 5000 pero la performance cayó notablemente ademas que los tiempos de procesamiento se volvieron inviables.
Teniendo en cuenta que al superar la barrera de los 1000 árboles los cambios en los resultados que arrojaba el clasificador cada vez eran menos significativos pero aumentaban en gran manera el costo computacional, se tomó el tercer caso como el ideal para la aplicación de este algoritmo debido al equilibrio que brinda entre efectividad y performance.